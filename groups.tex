% !TEX encoding = MacOSRoman
% Standalone document
\documentclass[notes.tex]{subfiles}
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Groups}
\label{chap:groups}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The study of symmetries plays a central part in theoretical physics, and the mathematical language we use is that of groups. The action of the group elements on our states or fields is what effects the transformation that has the symmetry, while the invariance of the physical properties of the system under that transformation is the symmetry itself. For example, rotations in three-dimensions can be carried out by the application of a $3\times3$ rotation matrix on the coordinates of an object. As we shall see, these matrices form the group called $SO(3)$. For a sphere, which is invariant under these rotations, $SO(3)$ is then the symmetry group.

Of special interest to us are the Lie groups, which are the groups that represent continuous transformations, such as the $SO(3)$ rotations. The properties of Lie groups can be further studied by finding their generators which form a (Lie) algebra. The generators {\it almost} -- in a very specific sense of the word almost -- describes the whole group, and allows us to reconstruct the group elements by what is called the exponential map. This we will return to in the next chapter.

Here, we will begin by defining groups and looking at some of their most important properties. What is crucial in physics are the {\it representations} of groups, meaning what the operators of the transformations on the states actually look like. Returning to the rotation example these are $3\times3$ matrices, but with some restrictions on their elements. After discussing representations we will move on to defining Lie groups, before we end on a discussion of their generators.


%%%%%%%%%%%%%%
\section{Group definition}
%%%%%%%%%%%%%%
A group is an abstract algebraic structure that consists of a set of objects (the elements), and an operation acting between pairs of these elements that gives another element in the set. In addition the set needs to contain an element that works as an identity, and every element in the group needs to have an inverse. Because these properties are very similar to the properties of multiplication with real numbers (or rationals), the operation is often called multiplication.

We define a group as follows.
\df{The set of elements $G = \{g_i\}$ and operation $\circ$ (multiplication) form a {\bf group} if and only if for all $g_i \in G$ :\begin{enumerate}[i)]
\item $g_i \circ g_j \in G$, \hfill (closure)
\item $(g_i \circ g_j)\circ g_k = g_i\circ(g_j \circ g_k)$, \hfill (associativity)
\item There exists an $e \in G$ such that $g_i \circ e = e\circ g_i = g_i$, \hfill (identity element)
\item There exists an $g_i^{-1} \in G$ such that $g_i \circ g_i^{-1} = g_i^{-1}\circ g_i = e$. \hfill (inverse)
\end{enumerate}}
Below, where no confusion can occur, we will often drop the multiplication symbol for the group multiplication, writing $g_i \circ g_j =g_ig_j$. We should also speak of the group as the pair $\{G,\circ\}$, however, we will often abusively refer to just the set $G$, with the multiplication operation being implied.
 
The real numbers sans zero, ${\mathbb R}\setminus \{0\}$, with ordinary multiplication as the operation $\circ$, and $e=1$ as the identity, is an obvious example of a group. Here we need to remove zero since it does not have an inverse. Another straight forward example of a group is $G= \mathbb{Z}$ (the integers), with standard addition as the operation $\circ$. Then $e = 0$ and $g^{-1} = -g$. Alternatively we can restrict the group to $\mathbb{Z}_n$, where the operation is now addition modulo $n$. In this group, $g_i^{-1} = n - g_i$, and the unit element is again $e = 0$.\footnote{Note that we here use $e$ for the identity in an abstract group, while we will later use $I$ or $1$ as the identity matrix in matrix representations of groups.} 

Here  $\mathbb{Z}$ is an example of an {\bf infinite} group, the set has an infinite number of members, while $\mathbb{Z}_n$ is {\bf finite}, with {\bf order} $n$, meaning $n$ members. Both are {\bf abelian} groups, meaning that the elements commute: $g_i \circ g_j = g_j \circ g_i$, because the standard addition commutes, but this is not a requirement for groups, and we will soon see non-abelian groups.

The simplest, non-trivial, of these $\mathbb{Z}_n$-groups is $\mathbb{Z}_2$ which only has the members $e=0$ and $1$. The ``multiplication'' operation is completely defined by the three possibilities
\begin{eqnarray*}
0+0 &=&0, \\ 0+1 &=& 1, \\ 1+1 &=&0.
\end{eqnarray*}
Now, compare this to the set $G=\{-1,1\}$ with the ordinary multiplication operation. Here, all the possible operations are 
\begin{eqnarray*}
1\cdot 1 &=& 1,\\  1\cdot (-1) &=&-1,\\ (-1)\cdot (-1) &=&1.
\end{eqnarray*}
This has exactly the same structure as $\mathbb{Z}_2$, only that the identity element is now $e=1$ and the other element is $-1$.  We say that these two groups are {\bf isomorphic}, because there is a one-to-one correspondence between all the (two) elements, $0\leftrightarrow 1$ and $1\leftrightarrow -1$, and the results of the multiplication operation is the same. In fact, we consider them as the {\it same group} despite the considerable apparent visual differences.\footnote{This observation  generalises to the set $G=\{e^{2\pi ik/n} | k=1,\ldots,n-1\}$, the {\bf $\mathbf n$-th roots of unity}, which, together with the standard multiplication operation, is isomorphic to  $\mathbb{Z}_n$.} 
This notion of isomorphic (``identical'') groups is very important, and we will return to it in more detail in Sec.~\ref{sec:group_properties}.

A somewhat more sophisticated example of a group can be found in the Taylor expansion of a function $F$, where
\begin{eqnarray*}
F(x+a) &=& F(x) + aF'(x) + \frac{1}{2} a^2 F''(x) + \ldots \\
&=& \sum_{n=0}^{\infty}\frac{a^n}{n!}\frac{\partial^n}{\partial x^n} F(x)\\
&=& e^{a\frac{\partial}{\partial x}} F(x).
\end{eqnarray*}
The last equality uses the formal definition of the exponential series, but may drive some mathematicians crazy.\footnote{We will not discuss this further, but there is a deep question here whether the operator formed by this exponentiation is well defined.}  

The resulting operator 
\begin{equation*}
T_a = e^{a\frac{\partial}{\partial x}},
\end{equation*} 
is called the {\bf translation operator}, in this case in one dimension, since it shifts the coordinate $x$ of the function $F$ it is operating on by an amount $a$. Defining the (natural) multiplication operation
$T_a \circ T_b =  T_{a+b}$
it forms the {\bf translational group} $T(1)$, where we can show that $T_0$ is the identity element and the inverse is $T_a^{-1} = T_{-a}$.\footnote{We could instead have defined the operation between two group elements to be ordinary multiplication and used that to show the relationship $T_a \circ T_b = T_{a+b}$. However, it is important to notice that showing this is not entirely trivial because ordinary arithmetic rules for exponentials fail for operators. In this particular case the proof is fairly simple, but this is in general not so. This will return to trouble us later in the notes.} Here, we call $a\in\mathbb R$ the {\bf parameterisation} of the group. 

In $n$ dimensions the group $T(n)$ has the elements  $T_{\mathbf{a}} = e^{\mathbf{a}\cdot \del}$, where $\del$ is the $n$-dimensional partial derivative vector, and the group is parameterised by the $n$-dimensional vector $\mathbf a$. We can of course also talk about the translation group in terms of how it acts on  finite-dimensional vector spaces, where the translation of a vector $\mathbf x\in \mathbb R^n$ is given by
\[ \mathbf x' =T_{\mathbf a} \mathbf x= \mathbf x + \mathbf a.\] 

Whereas we say that the groups $\mathbb{Z}$ and $\mathbb{Z}_n$ are {\bf discrete groups}, since we can count the number of elements, $T(n)$ is a {\bf continuous group} since the parameter $a$ can be any real number. 


%%%%%%%%%%%%%%
\section{Matrix groups}
\label{sec:matrix_groups}
%%%%%%%%%%%%%%
We next define some groups that are very important in physics and to the discussion in these notes. They have in common that they are defined in terms of square matrices.

%%%%%%%%%%%%%%%%%%%%%%%
\subsection{General and special linear groups}
The largest matrix group for a given matrix dimension $n$ is the general linear group.
\df{The {\bf general linear group} $GL(n)$ is defined by the set of all invertible $n\times n$ matrices $A$ under matrix multiplication. If we additionally require that $\det(A) = 1$, the matrices form the {\bf special linear group} $SL(n)$.}
Here, the closure property of groups is guaranteed by the fact that the product of two invertible matrices is also invertible, and matrix multiplication is always associative.
The existence of the group identity is guaranteed by the identity matrix $I$ being an invertible matrix (with $I$ as the inverse). Since the existence of an inverse is also necessary in the group definition, we cannot construct larger matrix groups. The general linear group also gives us our first example of a {\bf non-abelian group}, since matrix multiplication does not in general commute. For two matrices $A$ and $B$, we may have $AB\ne BA$.

The special linear group inherits all the properties of the general linear group. It is closed because the determinant of the product of two matrices, is the product of their determinants, {\it i.e.} if $A,B\in SL(n)$ then
\begin{equation*}
\det{AB}=\det{A}\cdot \det{B} =1 \cdot 1 = 1.
\end{equation*}
Also, the identity element is in $SL(n)$ since $\det{I}=1$.

We usually take the matrices in matrix groups to be defined over the field of complex numbers $\mathbb{C}$. If we want to specify the field we may use the notation $GL(n, \mathbb{R})$, signifying that the group is defined over the real numbers. Defined over the complex numbers the $GL(n)$ groups have $2n^2$ free parameters since each of the $n^2$ elements of the matrices can be a complex number, needing two parameters. The $SL(n)$ group has $2n^2-2$ free parameters since the requirement on the determinant fixes both the real and imaginary part of the determinant.

%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Unitary and special unitary groups}
\label{sec:unitary_groups}
We first remind you that the {\bf Hermitian conjugate} or conjugate transpose of a matrix is given by transposing the matrix and taking the complex conjugate of its elements. Here, we will use the dagger symbol $\dagger$ for this operation, so that for a matrix $A$, $A^\dagger=(A^T)^*$.

We now define the unitary groups.
\df{The {\bf unitary group} $U(n)$ is defined by the set of complex unitary $n\times n$ matrices $U$,
{\it i.e.}\ matrices such that $U^\dagger U = I$ or $U^{-1} = U^{\dagger}$. If we additionally require that $\det U = 1$ the matrices form the {\bf special unitary group} $SU(n)$.}
With these requirements it can be shown, see Ex.~\ref{ex:Un_parameters}, that the $U(n)$, groups have $n^2$ independent parameters, while the $SU(n)$ groups have $n^2-1$. Since $\det(I)=1$, for $U\in U(n)$ we have
\[\det(UU^\dagger)=\det(U)\det(U^\dagger)=\det(U)\det(U^T)^*=\det(U)\det(U)^*=|\det(U)|^2=1,\] 
so the determinant of the unitary matrices must be complex numbers on the unit circle, {\it i.e.}\ $\det(U)=e^{i\theta}$, where $\theta\in\mathbb R$.

It is these unitary groups that form the gauge symmetry groups of the Standard Model: $SU(3)$, $SU(2)$ and $U(1)$. The group $U(1)$ makes perfect sense despite the odd matrix dimensions. This is simply the set of all complex numbers of unit length with ordinary multiplication, {\it i.e.}\ $U(1)=\{e^{i\alpha} | \alpha \in\mathbb{R} \}$, but notice that $SU(1)$ would be trivial since it contains only the element 1. 

The members of the unitary group has has the important property that for all complex vectors $\mathbf{x}, \mathbf{y} \in \mathbb{C}^n$ -- think finite-dimensional quantum states or quantum fields -- multiplication by  a unitary matrix leaves scalar products (inner products) unchanged. If $\mathbf{x}'=U\mathbf{x}$ and $\mathbf{y}'=U\mathbf{y}$, then
\begin{eqnarray}
\mathbf{x}'\cdot \mathbf{y}' &\equiv& \mathbf{x}'{}^\dagger \mathbf{y}' = (U\mathbf{x})^\dagger U\mathbf{y}\nonumber\\
&=& \mathbf{x}^\dagger U^\dagger U\mathbf{y} = \mathbf{x}^\dagger \mathbf{y} = \mathbf{x} \cdot \mathbf{y}.\nonumber
\end{eqnarray}
Thus, as this implies $|\mathbf{x}'|=|\mathbf{x}|$, its members do not change the length of the vectors they act on.  Since we would like to let our group representations act on vectors that describe quantum mechanical states, the unitary groups then conserve probability for these states. For example, when acting on a complex number (a complex scalar), such as a wavefunction $\psi(x)$, the elements of $U(1)$ rotate the phase of $\psi$, however, the magnitude is conserved since $\psi'=e^{i\alpha}\psi$ gives $|\psi'|^2=\psi^*e^{-i\alpha}e^{i\alpha}\psi=|\psi|^2$.


%%%
\subsubsection{The matrix exponential}
The {\bf matrix exponential} interprets the exponential of a real or complex valued $n\times n$ matrix $M$ as the formal series
\[ \exp(M)\equiv\sum_{n=0}^\infty \frac{M^n}{n!}=I+M+\frac{1}{2}M^2+\frac{1}{6}M^3+\ldots. \]
This series can be shown to always converge, so the series is well defined. Since this is a series of non-commuting objects we again have to be careful with using properties of the exponential from ordinary arithmetic. 

The following very useful properties of the matrix exponential can be proven:
\begin{itemize}
\item[i)] $\exp(A^T)=\exp(A)^T$,
\item[ii)] $\exp(A^*)=\exp(A)^*$,
\item[iii)] If $B$ is an invertible matrix then $\exp(BAB^{-1})=B\exp(A)B^{-1}$,
\item[iv)] If $[A,B]=0$ then $\exp(A)\exp(B)=\exp(A+B)$,
\item[v)] $\det e^A = e^{\Tr A}$. 
\end{itemize}
% TODO: Discuss the use of Jordan canonical form in calculations here? Uses iii) to rewrite on Jordan canonical form $J=MAM^{-1}$ where $e^J$ is easy to evaluate because of the Jordan canonical form properties.
% Used in proofs?

We can construct unitary matrices from the exponential of either Hermitian or anti-Hermitian matrices in the following way. Given a Hermitian matrix $M$, $M^\dagger=M$, the matrix $U=e^{iM}$ is then automatically unitary since $U^\dagger=e^{-iM^\dagger}=e^{-iM}$ so that $U^\dagger U=e^{-iM}e^{iM}=I$, where the last equality is due to $M$ commuting with itself. We shall later in this chapter show that in fact all unitary matrices can be written in terms of Hermitian matrices like this. This is the physics construction that we will mostly use in these notes because of our quantum fondness for Hermitian operators. For an anti-Hermitian matrix $M$, $M^\dagger=-M$, we could instead use that $U=e^M$ is automatically unitary since $U^\dagger=e^{-M}$ so that $U^\dagger U=e^{-M}e^{M}=I$. This is the mathematical construction found in a lot of mathematical literature.

%%%
\subsubsection{$\mathbf{SU(2)}$}
A general member $S$ of $SU(2)$ can be written (parameterised) in terms of two complex parameters $\alpha,\beta\in\mathbb{C}$ as
\begin{equation}
S(\alpha,\beta)=\left[
\begin{matrix} \alpha & -\beta^* \\ \beta & \alpha^* \end{matrix}
\right],
\label{eq:SU2_parameterisation}
\end{equation}
with the additional constraint $|\alpha|^2+|\beta|^2=1$, leaving three free parameters as expected. 

There is a deep connection between the $SU(2)$ group and spin because the Hermitian {\bf Pauli matrices} 
\begin{equation}
\sigma^1 =\left[\begin{matrix} 0 & 1 \\ 1 & 0 \end{matrix}\right], 
\quad \sigma^2 =\left[\begin{matrix} 0 & -i \\ i & 0 \end{matrix}\right], 
\quad \sigma^3 =\left[\begin{matrix} 1 & 0 \\ 0 & -1 \end{matrix}\right],
\label{eq:pauli_matrices}
\end{equation}
that make up the spin-operators $S^i=\frac{\hbar}{2}\sigma^i$, of quantum mechanics, are unitary matrices -- thus members of $U(2)$ --  and, as we shall see, they can be used to generate (almost) any member of $SU(2)$ by the exponential $U=e^{i\alpha_i\sigma^i}$, where $\alpha_i\in\mathbb R$ are three real parameters.\footnote{Make a note of the notation here, the position of the indices is used to prepare the ground for using these in a four-vector $\sigma^\mu=(\sigma^0,\boldsymbol{\sigma})$ together with the identity matrix 
\[ \sigma^0=I_2=\left[ \begin{matrix}1 &0  \\ 0 & 1  \end{matrix} \right].\] }


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Orthogonal and special orthogonal groups}
If we restrict the unitary matrices further, so that their elements are all real,  we get the orthogonal groups.
\df{The {\bf orthogonal group} $O(n)$  is the group of real  $n\times n$  orthogonal matrices $O$, {\it i.e.}\ matrices where $O^TO=I$. If we additionally require that $\det(O) = 1$ the matrices form the {\bf special orthogonal group} $SO(n)$.}
It follows from the definition of the orthogonal group that the determinant of the members is either 1 or $-1$, thus the special orthogonal group is simply one half of the members. For $\mathbf{x} \in \mathbb{R}^n$ the orthogonal group has the same property as the unitary group of leaving the length of vectors invariant. 

Matrices in the $O(n)$ and $SO(n)$ groups have $n(n-1)/2$ independent parameters since an $n\times n$ matrix with real entries has $n^2$ elements, and there are $n(n+1)/2$ equations to be satisfied by the orthogonality condition.\footnote{Only the upper triangular part of $O^TO$ has independent equations since $O^TO$ is a symmetric matrix, $(O^TO)^T=O^T(O^T)^T=O^TO$. } 

The special orthogonal groups $SO(2)$ and $SO(3)$ are much used because their elements represent rotations in two and three dimensions, respectively, while $SO(n)$ extends this to higher dimensions and represents the symmetries of a sphere in $n$ dimensions. To see this we can start from the fact that rotations, by definition, conserve angles and distances (and orientation). This means that the original set of orthogonal axis -- or orthogonal basis vectors if you wish -- must transform into another orthogonal set of axis under the rotation. The matrix performing the rotation must then be orthogonal, and thus  the collection of rotations must be $O(n)$. If we additionally require that orientation is preserved, this removes the matrices with negative determinant, leaving the $SO(n)$ group.

%%%
\subsubsection{$\mathbf{SO(2)}$}
Given that $SO(2)$ has only one parameter, we can write a general group member $R$ as parameterised by $\theta$
\begin{equation}
R(\theta)=\left[\begin{matrix} \cos\theta &  -\sin\theta \\ \sin\theta&  \cos\theta \end{matrix}\right].
\label{eq:SO2_parameterisation}
\end{equation}
As expected, we recognise this as the matrix of rotations of an angle $\theta$ around a point in the plane, and it represents the (orientation preserving) symmetries of a circle. Some tinkering with this representation will show that $SO(2)$ is in fact an abelian group, despite the matrix definition. From a physical viewpoint this should be expected: the order of rotations in the plane should not matter.

It is interesting to observe that the elements of $SO(2)$ rotate points in the plane, while the elements of $U(1)$ rotate complex numbers, which can be represented by points in the plane. Indeed, a one-to-one correspondence can be found between the members of the two groups so that the groups are indeed the same, or, as we say, isomorphic, $SO(2)\cong U(1)$.

%%%
\subsubsection{$\mathbf{SO(3)}$}
This group has three free parameters. Already at this point writing down the explicit form of a general group member is not very enlightening. There are also a number of different conventions in use, so proper care is advised when using results from the literature. In terms of a general rotation in three dimensions this can either be viewed as rotation angles around three fixed axis, or as the fixing of a rotation axis by two angles, with a third rotation angle around that axis. 

One particular explicit form, where the angles correspond to the three {\bf Euler angles} of rotation in three dimensions, $\alpha$, $\beta$ and $\gamma$, is
\begin{equation}
R(\alpha,\beta,\gamma)=\left[\begin{matrix} 
\cos\alpha\cos\beta\cos\gamma-\sin\alpha\sin\gamma & -\cos\alpha\cos\beta\sin\gamma-\sin\alpha\cos\gamma & \cos\alpha\sin\beta  \\ 
\sin\alpha\cos\beta\cos\gamma+\cos\alpha\sin\gamma & -\sin\alpha\cos\beta\sin\gamma+\cos\alpha\cos\gamma & \sin\alpha\sin\beta  \\ 
-\sin\beta\cos\gamma & \sin\beta\sin\gamma  & \cos\beta 
\end{matrix}\right],
\label{eq:R3_rotation_Euler}
\end{equation}
where $0\le\alpha,\gamma< 2\pi$ and $0\le\beta\le\pi$. These rotations do not commute, so the group is non-abelian.

We have seen above that $SU(2)$ has three free parameters, just the same as $SO(3)$. You may at this point guess that $SO(3)$ is isomorphic to $SU(2)$. That would be a very good guess, however, it would also be wrong. We will return to this later, but this is one of those things in mathematics that turn out to be disappointingly only almost true.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Symplectic and compact symplectic groups$^*$}

\df{The {\bf symplectic group} $Sp(2n)$  is the group of $2n\times 2n$ symplectic matrices $M$, {\it i.e.}\ matrices where $M^T\Omega M=\Omega$, with
\[ \Omega= \left[ \begin{matrix} 0 & I_n \\ -I_n & 0 \end{matrix} \right].\]
The  {\bf compact symplectic group} $Sp(n)$ is the  intersection of the symplectic group $Sp(2n)$ and the unitary group $U(2n)$, {\it i.e.}\ 
\[ Sp(n)\equiv Sp(2n)\cap U(2n), \]
so that its matrices are members of both groups.\footnote{The $n$ as apposed to $2n$ here is intentional, the reason is that the compact symplectic group can be described in terms of the unitary group $U(n,\mathbb H)$ of $n\times n$ matrices over the quaternions (see Sec.~\ref{sec:normed_division_algebras}).} 
}

The choice here of $\Omega$ can be generalised to any nonsingular skew-symmetric matrix, {\it i.e.} an invertible matrix $\Omega$ where $\Omega^T=-\Omega$, however, this does not change the group structure.
Note that we can easily show $\det\Omega=1$ and $\Omega^{-1}=\Omega^T=-\Omega$ for our choice of $\Omega$. This in turn implies that the symplectic matrices have $\det M=\pm1$, but this can be strengthened to show that indeed $\det M=1$.

The compact symplectic group has interesting applications in classical mechanics. If a system of $n$ particles has generalised coordinates $q_i$ and momenta $p_i$ we can stack the coordinates in the vector $\boldsymbol \eta=(q_1,\ldots,q_n,p_1,\ldots,p_n)^T$. It can then be shown that any symplectic  matrix $M$ acting on $\boldsymbol\eta$ is a {\bf canonical transformation}, meaning it is a transformation that leaves Hamiltons equations unchanged, so the symplectic groups are the symmetry groups of Hamilton's equations.

In quantum mechanics we can write the canonical commutation relations $[\hat q_i, \hat p_j]=i\hbar\delta_{ij}$ as
\[ [\hat{\boldsymbol \eta}, \hat{\boldsymbol \eta}^T]=i\hbar\Omega. \]


%%%%%%%%%%%%%%
\section{Group properties}
\label{sec:group_properties}
%%%%%%%%%%%%%%

%%%%%%%%%%%%
\subsection{Subgroups}
We now extend our vocabulary for groups by defining the {\bf subgroup} of a group $G$.
\df{A subset $H \subset G$ is a {\bf subgroup} if and only if:\footnote{An alternative, equivalent, and more compact way of writing these two requirements is the single requirement $h_i \circ h_j^{-1} \in H$ for all $h_i, h_j\in H$. This is often utilised in proofs.}
\begin{enumerate}[i)]
\item $h_i \circ h_j \in H$  for all  $h_i,h_j\in H$, \hfill (closure)
\item $h_i^{-1} \in H$ for all $h_i\in H$. \hfill (inverse)
\end{enumerate}
$H$ is a {\bf proper} subgroup if and only if $H\neq G$ and $H \neq \{e\}$. }
We have already seen some examples of subgroups: the $SU(n)$ groups are subgroups of $U(n)$, and the $SO(n)$ groups are subgroups of the $O(n)$ groups. This can easily be shown using the properties of determinants.

There is a very important class of subgroup called the {\bf normal} subgroup. The importance will become clear in a moment.
\df{A subgroup $H$ is a {\bf normal} (invariant) subgroup, if and only if the {\bf conjugation} of any element $h\in H$ by any $g \in G$ is in $H$,\footnote{Another, pretty, but slightly abusive, way of writing the definition of a normal group is to say that $gHg^{-1}=H$. This implies (correctly), that the image if $H$ under the conjugation operation is guaranteed to be the whole of $H$.}
meaning
\[ ghg^{-1} \in H\text{ for all } h \in H.\]
A {\bf simple} group $G$ has no proper normal subgroup. A {\bf semi-simple} group $G$ has no proper abelian normal subgroup.}
We can for example show that for $n>1$, $SU(n)$ is a normal subgroup of $U(n)$, see Ex.~\ref{ex:SUn_normal}. 

%%%%%%%%%%%%%%
\subsection{Quotient groups}
The normal subgroup can be seen as a factor in the original group that can be divided out to form a simpler group that only retains the structure that was not in the normal group. To be more precise we need the concept of {\bf cosets}. 
\df{A {\bf left coset} of a subgroup $H\subset G$ with respect to $g\in G$ is the set of members $\{gh | h\in H\}$, and a {\bf right coset} of the subgroup is the set  $\{hg|h\in H\}$. These are sometimes written $gH$ and $Hg$, respectively.}
Despite the appearance of containing many of the same members we can show any two cosets are either disjoint or identical sets.

For normal subgroups $H$ it can be shown that the sets of left and right cosets $gH$ and $Hg$ coincide and form a group. This is called the {\bf quotient}  or {\bf coset group} and denoted $G/H$.\footnote{Sometimes also called the {\bf factor group}. The notation $G/H$ is pronounced ``G mod H'', where ``mod'' is short for modulo.}
This has as its members all the {\it distinct sets} $\{gh|h\in H\}$, that can be generated by a $g\in G$, and has the binary operation $*$ with $\{gh|h\in H\}*\{g'h|h\in H\}=\{ (g\circ g')h| h\in H\}$. To simplify notation this can be written $gH*g'H=(g\circ g') H$. 
 
To make some sense of this, let us briefly discuss an example of a quotient group. We already know that $SU(n)$ is a normal subgroup to $U(n)$. This means that $U(n)/SU(n)$ is a group. What sort of group is this? Notice that two matrices $U_i$ and $U_j$ live in the same coset of $SU(n)$ if and only if $\det(U_i)=\det(U_j)$. In other words, each coset constructed from $SU(n)$ is simply the set of all matrices with a given determinant, which we saw for the $U(n)$ group can be any unit complex number. Observe that this means that many of the cosets are the same, all cosets generated by members in $U(n)$ with the same determinant is the same coset. We can even label each of the cosets with the determinant if we wish.
When these cosets act on each other with the group operation of $U(n)/SU(n)$ they form new cosets of matrices with a determinant that is the product of their individual determinants, {\it i.e.}\ with $U,U'\in U(n)$ we have the product of quotient group member $\{US|S\in SU(n)\}*\{U'S| S\in SU(n)\}=\{ UU'S| S\in SU(n)\}$, where the result of the product is labeled by the determinant of $UU'$. Thus, the group behaves exactly as $U(1)$, and is in fact isomorphic to it.

%%%%%%%%%%%%%%
\subsection{Product groups}
Now that we have introduced group division,  we also need to introduce products of groups. 
\df{The {\bf direct product} $G\times H$ of groups $G$ and $H$  is defined as the ordered pairs $(g,h)$ where $g\in G$ and $h\in H$, with component-wise operation $(g_i,h_i)\circ(g_j, h_j) = (g_i\circ g_j , h_i \circ h_j)$. $G\times H$ is then a group and $G$ and $H$ can be shown to be normal subgroups of $G\times H$.}
We should note here that the subgroups are strictly $G\times\{e_H\}$ and $\{e_G\}\times H$, but these are isomorphic to $G$ and $H$. 

The famous Standard Model gauge group $SU(3) \times SU(2)\times U(1)$ is an example of a direct product. Direct products are ``trivial" structures because there is no ``interaction" between the subgroups, the elements of each group act only on elements of the same group. For the Standard Model this means that the each of the gauge transformations can act independently on the states unaffected by any other gauge transformation. 

We will also need the definition of the {\bf semi-direct product}.
\df{The {\bf semi-direct product} $G\rtimes H$ of groups $G$ and $H$,  where $G$ is also a mapping $G:H\to H$, is defined by the ordered pairs $(g,h)$ where $g\in G$ and $h\in H$, with component-wise operation \[(g_i,h_i)\circ(g_j, h_j) = (g_i\circ g_j , h_i \circ g_i(h_j)).\] Here $H$ is not a normal subgroup of $G\rtimes H$, but $G$ is.}
Note how the semi-direct product is not symmetric between the factors, the members of $G$ change the members of $H$ in the multiplication.

An example of a semi-direct product is found in the {\bf Euclidean group} $E(n)$ which is the semi-direct product of the translation group $T(n)$ and the orthogonal group $O(n)$, $E(n) = T(n)\rtimes O(n)$.  Physically, the Euclidean group is the group of all rotations, translations and reflections in $n$-dimensional Euclidean space $\mathbb {E}^n$. These are the transformations that preserve the distance between two points in that space, and we say that it is the (coordinate/external) symmetry group of the space. 

We can see how the semi-direct product works by writing down how coordinates change under the transformations of the Euclidean group. A vector $\mathbf x \in \mathbb {E}^n$ would be transformed as
\[ \mathbf x' = O\mathbf x + \mathbf a,\]
where $O$ is a matrix in $O(n)$ that performs rotations and reflections, and $\mathbf a$ is a constant vector that performs the translation of the vector. If we now do two Euclidean transformations with $(O_j,\mathbf a_j)$ and $(O_i,\mathbf a_i)$ we get the total transformation
\[ \mathbf x' = O_i(O_j\mathbf x + \mathbf a_j)+\mathbf a_i= O_iO_j\mathbf x + O_i\mathbf a_j+\mathbf a_i,\]
which is equivalent to the Euclidean transformation with parameters $(O_iO_j,\mathbf a_i  +O_i\mathbf a_j)$.

If we specialise by taking away the reflections, we define the {\bf special Euclidean group} $SE(n)= T(n)\rtimes SO(n)$.  


%%%%%%%%%%%%%%%%
\subsection{Isomorphic groups}
We have already talked about how two groups are the same if they have a one-to-one correspondence between their members and the same results for the multiplication operation, and we have called this an isometry. Let us now try to put this notion of when two groups are the same into a more formal language.
\df{Two groups $G$ and $H$ are {\bf homomorphic} if there exists a map between the elements of the groups $\rho:G\to H$, such that for all $g,g'\in G$,  $\rho(g\circ  g')=\rho(g)\circ \rho(g')$.}
For homomorphic groups we say that the mapping $\rho$ conserves the structure of the group, or in other words, all the rules for the group operation/multiplication. This leads directly to our notion of group equality, namely {\bf isomorphic} groups:
\df{Two groups $G$ and $H$ are {\bf isomomorphic}, written $G\cong H$, if they are homomorphic and the relevant mapping is one-to-one (injective) and onto (surjective).}
The one-to-one and onto (hitting all elements of $H$) mapping ensures that there is a one-to-one correspondence between the elements of the two groups, so that isomorphic groups effectively contain both the same members and have the same multiplication operation.

Let us look at an example with the isomorphism $U(n)/SU(n)\cong U(1)$. For matrix groups, a way of checking the plausibility of isomorphism is to count the number of free parameters.  In our example $U(n)/SU(n)$, the difference of the number of parameters for the two factors should be equal to the number of parameters for the quotient group. Here, $U(n)$ has $n^2$ parameters, while $SU(n)$ has $n^2-1$, and this gives $n^2-(n^2-1)=1$ parameter for the quotient group, which matches the one parameter of $U(1)$. To demonstrate the isomorphism we need to find the mapping between the groups. For the quotient group each member of $U(n)/SU(n)$ is the coset $\{US|S\in SU(n)\}$, while each member of $U(1)$ can be written as $e^{i\theta}$. If we then make the map $\rho(\{US|S\in SU(n)\})=\det U$, then this is homomorphic since
\begin{align*}
\rho(\{US|S\in SU(n)\}*\{U'S| S\in SU(n)\}) &= \rho( \{ UU'S| S\in SU(n)\} ) \\ 
& = \det UU'  \\
&= \det U \det U' \\
&= \rho(\{US|S\in SU(n)\})\rho(\{U'S| S\in SU(n)\}),
\end{align*}
and isomorphic since the determinants of unitary matrices are numbers on the unit circle in the complex plane, and the cosets can be identified uniquely with the determinant of the matrix that generate them.

Another example is the isomorphism $SO(2)\cong U(1)$. Here the elements $R\in SO(2)$ are the matrices in (\ref{eq:SO2_parameterisation}). The map is then simply $\rho(R)= e^{i\theta}$. This is homomorphic since
\[ \rho(R_iR_j)= e^{i(\theta_i+\theta_j)}= e^{i\theta_i} e^{i\theta_j} =\rho(R_i)\rho(R_j), \]
where we have used that we can just add up rotations in the plane in the first equality. It is isomorphic since all rotation angles correspond to one, and only one, number on the complex unit circle.

At the end of this subsection, let us make a warning: despite the enticing notation it is {\it not} in general the case that if $H$ is a normal subgroup to $G$, then $G\cong G/H\times H$.


%%%%%%%%%%%%%
\section{Representations}
\label{sec:rep}
%%%%%%%%%%%%%
In some contrast to the treatment in most introductory group theory texts in mathematics, physicists are mostly interested in the properties of groups $G$ where the elements of $G$ {\it act} to transform some elements of a vector space $v\in V$, $g(v) = v' \in V$. We have seen above how the matrix group members could act on finite-dimensional vector spaces.  This part of group theory is called {\bf representation theory}. Here, the members of $V$ can for example be the state of a system, say a wave-function in quantum mechanics or a field in quantum field theory. 
To be useful to us in physics, we would like that the result of the group operation $g_i \circ g_j$ acts on the vector space as $(g_i\circ g_j)(v) = g_i(g_j(v))$, and the group identity acts as $e(v) = v$.


%%%%%%%%%%%
\subsection{Definition}
We begin with the abstract definition of a representation that ensures these properties. 
\df{A {\bf representation} of a group $G$ on a vector space $V$ over the field $K$ is a map $\rho : G \to GL(V,K)$, where $GL(V,K)$ is the {\bf general linear group of the vector space} $V$,\footnote{The general linear group of the abstract vector space $V$ is the set of all one-to-one and onto linear transformations $V\to V$ (transformations that preserve vector addition and scalar multiplication), together with functional composition as group operation. For finite dimensional spaces this group is isomorphic to the general linear group $GL(n,K)$ that we introduced earlier.} such that for all $ g_i,g_j\in G$,
\begin{equation}
\rho(g_i\circ g_j) = \rho(g_i)\rho(g_j). \text{    (homomorphism)}\nonumber
\end{equation}
If this map is also isomorphic, we say that the representation is {\bf faithful}.
}
Here $V$ is called the {\bf representation space} and the dimension $n$ of $V$ is called the dimension of the representation. Somewhat confusing, it is common to talk about $V$ as the representation itself, even though it is really the space on which the representation acts. 

To see that this definition fulfils our wanted properties, notice that $\rho(g)=\rho(e\circ g)=\rho(e)\rho(g)$, which means that $\rho(e)= \mathbb{1}$ must be the identity transformation on $V$. The concatenation property follows directly from the homomorphism.

Writing this definition in terms of a generic vector space $V$ and field $K$ may be a bit obfuscating, but we need to keep in mind that our group elements can be acting on members of abstract vector space, {\it e.g.}\ spaces where the members of the space are functions, such as in the earlier example of the translation group acting on functions. In the case where $V$ is finite dimensional it is common to choose a concrete basis for $V$ and identify $GL(V,K)$ with $GL(n, K)$, the group of $n\times n$ invertible matrices with elements from the field $K$. So, in many cases, the representations we are interested in are matrices acting on coordinate space vectors over the fields $\mathbb R$ or $\mathbb C$, {\it i.e.} vectors in $\mathbb R^n$ or $\mathbb C^n$.

From a physics point of view, the underlying point here is that (the members of) our groups will be used on quantum mechanical states, or fields in field theory, which can be just complex numbers or functions, or multi-component vectors of such. They are thus members of a vector space, and the definition of representations allows the transformation properties of the group to be written in terms of matrices for finite dimensional representation spaces. Furthermore, the mapping from the group, or, if you like, the concrete way of writing the abstract group elements, must be homomorphic (structure preserving), meaning that if we can write a group element as the product of two others, the matrix for that element must be the product of the two matrices for the individual group elements it can be written in terms of.

%% TODO: Separate section here on unitary representations? Relate to compact. Existence of finite-dimensional
%% TODO: Discuss the difference between active and passive view of transformation, e.g. Williams Sec. 1.2
% TODO: Linear representations as a special category

You may by now have realised that the matrix groups defined in Sec.~\ref{sec:matrix_groups} have the property that they are defined in terms of one of their representations. These are called the {\bf fundamental} or {\bf defining  representations}. However, we will also have use for other representations, {\it e.g.}\ the {\bf adjoint representation} which we will introduce later.


The existence of multiple representations for the same group necessitates a definition of when representations are actually equivalent, or isomorphic. This should not be confused with whether groups are isomorphic, but removes differences in representations that are simply due to a change in basis for the vector space the group is acting on, or trivial changes in the dimension of the vector space.\footnote{Imagine, for example, that you create a representation for $U(1)$ that consists of diagonal $2\times2$-matrices with the unit complex numbers repeated twice on the diagonal. This is not essentially different from the one-dimensional representation, and should not be considered as such.} We again give the definition for an abstract representation space.  
\df{Two representations $\rho$ and $\rho'$ of $G$ on $V$ and $V'$ are {\bf equivalent} if and only if there exists a map $A:V\to V'$, that is one-to-one and onto, such that for all $g \in G$, $A\rho(g)A^{-1} = \rho'(g)$.}
For a finite-dimensional space $V=V'$ with a given basis, $A$ and $\rho(g)$ are simply matrices, and the transformation $A\rho(g)A^{-1} $ is a {\bf similarity transformation}, where $A$ is the change-of-basis matrix that transforms between two different bases for $V$.


%%%%%%%%%%%%%%%%%%%
\subsection{Representation examples}

Let us now take a few examples that connect to our definition and to relevant physics. We saw earlier that for $U(1)$ the group members can be written in the fundamental representation as the complex numbers on the unit circle $e^{i \theta}$, which can be used as phase transformations on wavefunctions $\psi(x)$, $\psi\to\psi'=e^{i\theta}\psi$. This can be viewed as the representation on a one-dimensional vector space over $\mathbb C$ where the basis vector is the wavefunction.

In physics an important category of representations are the {\bf unitary representations}, which are representations $\rho$ of a group $G$ on a complex Hilbert space $V$ where $\rho(g)$ is a unitary operator for every $g\in G$, {\it i.e.}\ an operator that preserves the length of vectors in $V$.\footnote{As a Hilbert space the space is an inner product space, so it has a notion of vector length.} This generalises the unitary (matrix) group, which are examples of unitary representations, beyond finite dimensional vector spaces.


For $SU(2)$ we saw that we needed three real numbers to parametrise the group elements. It then seems reasonable that we should be able to write a general matrix in $SU(2)$ in terms of the linear combination of three unitary ``basis'' matrices, for example the {\bf Pauli matrices} in (\ref{eq:pauli_matrices}). Since the three Pauli matrices are linearly independent, the sum $\alpha_i\sigma^i$, $\alpha_i\in\mathbb R$, should (hopefully) in some sense span all of $SU(2)$.\footnote{We will later see to what extent this is true, but we must emphasise here that $SU(2)$ is not a vector space, so it is not spanning in the vector space sense.} As it turns out, the group elements of $SU(2)$ in the fundamental representation can indeed written as the exponentials  $\exp(i \alpha_i \sigma^i)$. We will return to why we use an exponential in Sec.~\ref{sec:lie_algebras}. In physics the fundamental representation of $SU(2)$ is often denoted {\bf 2}, following the pattern that the representations are written up with the dimension of their matrices in boldface. In the Standard Model the fundamental representation is used on the vector space of doublets of fermion fields, {\it e.g.}\ the electron--neutrino doublet $\psi = (\nu_e, e)^T$ that forms a two-dimensional vector space, as the $SU(2)_L$ gauge transformation. 

However, we can construct many more representations than the fundamental from a single group such as $SU(2)$. Using the three free parameters in $SU(2)$ it turns out that we can also represent the elements in the group in terms of three  $3\times3$-matrices that act on vectors in a three-dimensional space. In place of the Pauli matrices we can use the three (Hermitian) matrices
\begin{equation}
L^1 =i\left[\begin{matrix} 0 & 0 & 0 \\ 0 & 0 & -1 \\ 0 & 1 & 0  \end{matrix}\right], 
\quad L^2 =i\left[\begin{matrix} 0 & 0 & 1 \\ 0 & 0 & 0 \\ -1 & 0 & 0  \end{matrix}\right], 
\quad L^3 =i\left[\begin{matrix} 0 & -1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 0  \end{matrix}\right],
\label{eq:SO3_generators}
\end{equation}
and write an element in $SU(2)$ as $\exp(i\alpha_i L^i)$.
You may recognise these as related to the angular momentum operators in quantum mechanics in the Heisenberg picture. These create the three-dimensional adjoint representation of $SU(2)$, named {\bf 3}. In the Standard Model this is the representation used for operations on the gauge fields that transform under $SU(2)_L$, which are represented as three-component vectors. The central point here is that the group structure is the same (isometric), even if the objects in the representation are different.

At this point you may be worried about the translation group $T(1)$ that we saw earlier and its extensions to higher dimensions. There we wrote down the elements using differential operators $T_a=e^{a\frac{\partial}{\partial x}}$. This does not look a lot like matrices. However, here the abstract vector space that is acted on by the group elements is made up of smooth (infinitely differentiable) functions, and in our generic definition of a representation above there is no need for the general linear group of such a space to be written in terms of matrices. Unless we are very strict with what functions we allow, this is typically an infinite dimensional space. It is a bit tricky to prove that our differential operators $T_a$ form a representation, however, they do, and these kinds of group representations are called {\bf differential representations}.



%%%%%%%%%%%%%%%%%%%%
\subsection{Irreducible representations}
\label{sec:irreps}
The building blocks of representations are so-called {\bf irreducible representations}, also called {\bf irreps}. These are the essential ingredients in representation theory, and are defined as follows: 
\df{An {\bf irreducible representation} $\rho$ of a group $G$ on $V$ is a representation where there is {\it no} proper subspace $W \subset V$ that is closed under the group, {\it i.e.}\ there is no $W\subset V$ such that for all $w \in W$, and all $g \in G$ we have $\rho(g)w \in W$.}
For finite dimensional representations this says that the matrix representation can not be split into two parts (blocks in the matrix) that do not  ``mix''.

Let us take an example to try to clear up what a {\it reducible} representation means in contrast to an irreducible. Assume the representation $\rho(g)$ for $g \in G$ acts on a vector space $V$ as matrices. If these matrices $\rho(g)$ can all be decomposed into $\rho_1(g)$, $\rho_2(g)$ and $\rho_{12}(g)$ such that for $\mathbf v =(\mathbf v_1, \mathbf v_2)^T\in V$ 
\[\rho(g) \mathbf v = \begin{bmatrix}\rho_1(g) & \rho_{12}(g) \\ 0 & \rho_2(g)\end{bmatrix} \begin{bmatrix}\mathbf v_1 \\ \mathbf v_2 \end{bmatrix}  ,\]
then $\rho$ is {\bf reducible}. The subspace $W$ of $V$ spanned by $\mathbf v_1$ violates the irreducibility condition above.\footnote{We here assume that the vector from $V$ is so organised that the subspace $W$ is conveniently located at the ``top'' of the vector, but this need not be the case in general. However, remember that we defined representations as equivalent under similarity transforms. This means that we are free to switch the order of the entries in the vector in such a way as to separate the subspaces neatly into blocks, while the representation is still the same representation.}

If  we also have $\rho_{12}(g)=0$ in our example we say that the representation is {\bf completely reducible}. It can be shown that in most cases a reducible representation is also completely reducible. In fact, representations for which this is not true tend to be mathematical curiosities. For example, if the representation is unitary and the vector space is a Hilbert space (states in quantum mechanics), we can prove that the representation is always completely reducible.  As a result, there is a tendency in physics to use the term ``reducible" where we should maybe use the term ``completely reducible". 

In the case of a completely reducible representation we can split the vector space $V$ into a direct sum of two vector spaces $V=V_1\oplus V_2$, where $\mathbf v_1\in V_1$ and $\mathbf v_2\in V_2$, and define a representation of $G$ on each of them using $\rho_1$ and $\rho_2$, which in turn could either be reduced more, or would themselves be irreducible. This process can then be continued until the representation has been broken down into only irreducible components.

We end this section with an important theorem that helps us decide whether a representation is irreducible, and ultimately gives a property identifying the representation. As with many important theorems, it started its life as a lemma.
\theo{{\bf (Schur's Lemma~\cite{Schur})}\\ If we have an irreducible representation $\rho$ of a group $G$ on a vector space $V$ and  a linear operator $A:V\to V$ that commutes with $\rho(g)$ for all $g \in G$, then  $A$ is proportional to the identity map, $A=\lambda \mathbb{1}$. The constant of proportionality $\lambda$ can be used to label the representation.}

%\bigskip
%{\it Proof:} First we will see that if $A$ is not trivial, meaning $A=0$, then $\det A\neq 0$.
%
%
%Let $V$ be the carrier space of $\rho(g)$. This means that any $\rho(g)$ acting on any element of $V$ produces another element of $V$. We introduce this in an obvious shorthand notation, $M V = V$. Let us begin by assuming that $A$ operating on elements of $V$ always produce an element in the subspace $V'\subset V$, $AV = V'$. This indirectly implies that $\det A = 0$ since otherwise
%
% If $A$ commutes with $M$, $[M,A]=0$, then 
%\[(MA)V = (AM)V,\]
%so
%\[MV' =AV =V'.\]
%Thus $M$ operating on $V'$ produces another vector in $V'$, so $V'$ must be an invariant subspace of $V$, which is inconsistent with our the assumption that $M$ is irreducible. This contradiction could be avoided if either $A$ is trivial,  $A=0$, or if $\det(A) \ne 0$, because then $V$ would be identical to $V'$. 
%
%Since we must have $\det A\neq 0$, we can write $A=B+\lambda I$ where $\det B=0$
%In this case we could still write $A=B+\lambda I$, where $\det(B)=0$ and $\det(A)=\lambda$. Then $[M,A]=0$ implies $[M,B] = 0$; and the only remaining way to avoid the contradiction is to admit that $B = 0$, and consequently $A$ is a multiple of the unit matrix.
%

Schur's lemma as stated here has again been formulated in the language of general vector spaces. In terms of matrix representations it says that any matrix $A$ that commutes with all the matrices in an irreducible representation must be equal to a constant times the identity matrix, {\it i.e.}\ $A=\lambda I$.

The constant of proportionality $\lambda$ is unique to the representation up to a common normalisation constant that can be incorporated into the linear operator/matrix (a constant multiple of $A$ would still have the same commutation properties). For us, these linear operators $A$ will generally turn out to have a very specific physical interpretation, so their natural normalisation will be clear. As a result the constants can be used to index the different irreducible representations. The proof of this property is somewhat beyond the scope of these notes.



%%%%%%%%%%
\section{Lie groups}
\label{sec:Lie_groups}
%%%%%%%%%%


%%%
\subsection{Definitions}
\label{sec:Lie_group_definitions}
In physics we are particularly interested in a special type of continuous groups that we can parametrise, the {\bf Lie groups}, which are the basic tools we use to describe continuous symmetries. The abstract mathematical notion of continuity comes from topology, along with a notion of open sets and proximity, so continuous groups are also called {\bf topological groups}. For topological groups the group multiplication and inverse need to be continuous maps. We further say that a topological group is a {\bf compact group} if its topology is compact, meaning that it has no punctures or missing endpoints, {\it i.e.}\ it includes all limiting values of the group members. In practise the topology of our groups will be subspaces of Euclidean space $\mathbb E^n$, in which case the space is compact if and only if it is closed and bounded. We will return to some examples of this later.

A Lie group extends the continuity of topological groups by requiring the multiplication and inverse maps to be {\it smooth} (infinitely differentiable $C^\infty$).
In order to define Lie groups we will need to use the technical term {\bf (smooth) manifold}, meaning a mathematical object (formally a topological space) that locally\footnote{This insistence on local means that the parameterisation is not necessarily the same for the whole group.} can be parametrised as a function of  $\mathbb{R}^n$ or $\mathbb{C}^n$. We will thus describe a Lie group $G$ as a manifold in terms of a parameterisation of the members $g(\mathbf a)\in G$, where $\mathbf a\in \mathbb{R}^n$ (or $\mathbb{C}^n$). Additionally, in order to describe continuous symmetries these parameterisations need to be smooth.
\df{A {\bf Lie group} $G$ is a finite-dimensional {\bf smooth manifold} where group multiplication and inversion are smooth functions, meaning that given elements $g(\mathbf{a}), g(\mathbf{a}') \in G$, $g(\mathbf{a})\circ g(\mathbf{a}') = g(\mathbf{b})$ where $\mathbf{b}(\mathbf{a}, \mathbf{a}')$ is a smooth function of $\mathbf a$ and $\mathbf a'$, and $g^{-1}(\mathbf{a}) = g(\mathbf{b})$ where $\mathbf{b}(\mathbf{a})$ is a smooth function of $\mathbf a$.}
The dimension of a Lie group is the dimension $n$ (or $2n$) of the manifold.

From our earlier example we immediately see that the translation group $T(1)$, given the parameterisation of the elements $g(a) = e^{a\frac{\partial}{\partial x}}$, is a Lie group since $g(a)g(a') =g(b)= g(a+a')$ and $b(a,a')=a+a'$ is an analytic function of $a$ and $a'$, and for the inverse $g^{-1}(a)=g(b)=g(-a)$ where $b(a)=-a$ is a smooth function of $a$. Since $a\in\mathbb R$ is unbounded the group members are also unbounded, and thus the group is not compact.

All the matrix groups are also Lie groups. While their dimension and parameterisation is clear, for example a member of $GL(n,\mathbb R)$ has $n^2$ parameters by its elements, to prove that they are in fact Lie groups is tricky because of the requirement on the non-zero determinant. The proof is done by showing it for the general linear group, and then using the {\bf closed subgroup theorem}, saying that any closed (in the topological sense) subgroup of a Lie group is a Lie group, which applies to the other matrix groups. 

We saw several examples of parameterisations of matrix groups earlier, for example $U(1)$ could be parameterised by a single parameter $\theta\in [0,2\pi]$ by writing an element $U\in U(1)$ as the complex number $U=e^{i\theta}$. We can then identify the group topologically with a subspace of $\mathbb E^2$, and since $|U|=1$ this group is bounded and closed, and thus a compact group. Similar arguments can be made for the compactness of all the matrix groups $U(n)$, $SU(n)$, $O(n)$, $SO(n)$, $Sp(2n)$, and $Sp(n)$, however, $GL(n)$ and $SL(n)$ are not compact because their elements are not bounded.


%%%%%%%%%%%%
\subsection{Generators}
\label{sec:generators}

The situation we are usually in in physics is that of a $d$-dimensional Lie group $G$ acting on a vector space $V$ through a representation. This representation vector space  may be finite dimensional, in which case its members $\mathbf x\in V$ are written as vectors in some specific basis, or it may be infinite dimensional and consist of functions that in turn have a domain $\mathbb R^n$ or $\mathbb C^n$.

For finite-dimensional representations we can locally write the map of the representation $G\times V \to V$ for $\mathbf{x} \in V$ in terms of an explicit function $\mathbf f$ called the {\bf composition function}. We have $x_i \to x_i' = f_i(\mathbf x, \mathbf a)$, $i=1, \ldots,n$, where the composition function $f_i$ is analytic\footnote{Analytic means infinitely differentiable and in possession of a  convergent Taylor expansion. As a result analytic functions (on $\mathbb{R}$) are smooth, but the reverse does not hold.} in $x_i$ and $a_j$, $j=1,\ldots,d$. Additionally $f_i$ should have an inverse. Note here that the dimensions of $\mathbf x$ and $\mathbf a$ are in general different, the first is given by the dimensions of the space $V$, $n$, while the second is given by the dimensions $d$ of the Lie group.

By the analyticity of the explicit function $f$ we can always construct the parametrisation so that the zero parameter corresponds to the identity element of the group, $g(0) = e$, which means that  $f_i(\mathbf x, 0)=x_i$.\footnote{If this was not true for our parameterisation we could Taylor expand $\mathbf f$ around the parameter giving the identity element and then redefine the parameterisation by a linear shift.} By an infinitesimal change $d\mathbf a$ of the  parameters we then get the following Taylor expansion\footnote{The fact that $f_i$ is analytic means that this Taylor expansion must converge in some radius around $f_i(\mathbf x,0)$.}
\begin{eqnarray*}
x'_i &=& x_i + dx_i = f_i(\mathbf x, d\mathbf a)\\
&=& f_i(\mathbf x, 0) + \left.\frac{\partial f_i}{\partial a_j}da_j\right|_{\mathbf a =0} +\ldots\\
&=& x_i +\left.da_j \frac{\partial f_i}{\partial a_j}\right|_{\mathbf a =0}.
\end{eqnarray*}
This is the result of the transformation by the member of the group that in the parameterisation sits $d\mathbf a$ from the identity. For a group where the matrix $A$ of the representation is parametrised as $A(\mathbf a)$ so that $\mathbf f = A(\mathbf a)\mathbf x$, the resulting change in the vector space is
\[
dx_i=\left.da_j\frac{\partial f_i}{\partial a_j}\right|_{\mathbf a =0}=da_j\left[\frac{\partial A(\mathbf a)}{\partial a_j}\mathbf{x}\right]_{i,\mathbf a=0}=[ida_jX^j\mathbf{x}]_i,
\]
where we have defined 
\[
iX^j\equiv \left. \frac{\partial A(\mathbf a)}{\partial a_j}\right|_{\mathbf a=0} ,
\]
as the $d$ matrix {\bf generators} $X^j$ of the Lie group.

If we now instead let $F(\mathbf x)$ be a function taken from the (infinite dimensional) vector space $V$ that we are interested in, mapping to either the real $\mathbb{R}$ or complex numbers $\mathbb{C}$, where $\mathbf x$ is now in the $n$-dimensional domain of the function, and letting $\mathbf f$ be the transformation of the domain effected by the group elements, then the group transformation defined by $d\mathbf a$ near the identity changes $F$  by
\begin{eqnarray*}
dF &=& \frac{\partial F}{\partial x_i}dx_i = \frac{\partial F}{\partial x_i}\left.\frac{\partial f_i}{\partial a_j}\right|_{\mathbf a=0}da_j\\
&\equiv& i da_j X^j F,
\end{eqnarray*}
where the operators defined by 
\begin{equation}
iX^j \equiv\left.\frac{\partial f_i}{\partial a_j} \right|_{\mathbf a=0}\frac{\partial }{\partial x_i},
\end{equation}
are called the $d$ differential {\bf generators} of the Lie group. We see here that in both cases the number of generators is the same as the dimension of the manifold, which is also the number of free parameters in the parameterisation of the group.

It is these generators $X$ that then define the effect of the Lie group members in a given representation (near the zero parameter), while the $d$ numbers $a_j$ are mere parameters. We say that the generators determine the local structure of the group. It turns out that this linearisation, often called the {\bf tangent space}, of a group using an infinitesimal change near the origin is sufficient to recover the whole group locally. Note that the generators are not unique, they do depend on the parameterisation of the group, but as we shall see later some of their properties are independent of this.

For the translation group $T(1)$ the action of the group on the domain $\mathbb{R}^1$ of the functions in the representation space, is $x'=f(x,a) = x+a$. The resulting (single) generator is
\[ iX^1=\frac{\partial f}{\partial a} \frac{\partial}{\partial x}= \frac{\partial}{\partial x}. \]
Notice here how we represented a generic element of $T(1)$ by the parameter and the generator as
\[ T_a=e^{ia_j X^j}=e^{a \frac{\partial}{\partial x}}. \]
This is of course no coincidence, and shows the way to a general recipe that we will use.\footnote{While we are in the business of {\it deja vu}, notice also how the generator for translations is $X=-i \frac{\partial}{\partial x}$, which can be compared with the quantum mechanical momentum operator $\hat p=-i\hbar \frac{\partial}{\partial x}$, keeping in mind that the conservation of momentum through Noether's theorem is intimately linked to the invariance of our models under translation. This is a point we will return to later.}

As another example of the above we can now go in the opposite direction and look at the two-parameter coordinate transformation {\it defined} by
\[x' = f(x) = a_1x + a_2,\]
which gives the generators
\[iX^1 = \frac{\partial f}{\partial a_1} \frac{\partial}{\partial x} = x\frac{\partial}{\partial x},\]
which is the generator for {\bf dilation} (scale change) in one dimension $D(1)$, and
\[iX^2 =  \frac{\partial f}{\partial a_2} \frac{\partial}{\partial x} =\frac{\partial}{\partial x},\]
which is again the generator for the translation $T(1)$. 
Notice that we can show the following relationship for these two generators: $[X^1, X^2] \equiv X^1X^2-X^2X^1= iX^2$. This can be seen to hold independently of the parameterisation of the group, something we will generalise later.

As we have seen, the group $SU(2)$ has three free parameters $a_i$, so it must have three generators $X^i$. We can now show that the generators for $SU(2)$ in the two-dimensional representation are indeed proportional to the Pauli matrices in (\ref{eq:pauli_matrices}) as intimated earlier, namely $X^i=\frac{1}{2}\sigma^i$, see Ex.~\ref{ex:SU2_generators}. By multiplying out we can also show the following commutation relationships between the generators (Pauli matrices):
\begin{equation}
[X_i,X_j]=i\epsilon_{ijk}X^k.
\label{eq:su2_algebra}
\end{equation}
These commutators should look familiar to us as they have the same structure as the commutators for the spin $S_i$ operators in quantum mechanics.\footnote{This would be $[S_i,S_j]=i\hbar\epsilon_{ijk}S^k$.} 

For matrix groups there is an alternative method to the generating functions for finding the generators that sees a lot of use. Since we are looking for infinitesimal deviations from the identity (matrix) $I$, we add an infinitesimal matrix $dM$ (the generator) and look at the properties of $dM$ given the definition of the group. Let us take $SO(2)$ as an example. Here a matrix in the group $O$ must fulfil $O^TO=I$ and $\det O=1$. Writing near the identity $O=I+dM$ we get the requirement $O^TO=I+dM+dM^T=I$ to first order in the infinitesimal. Thus the generator must fulfil $dM=-dM^T$ (it is a real, anti-symmetric matrix). This means that the diagonal of $dM$ must be zero, and in two dimensions, up to a constant, the only matrix that fits the role of the generator is
\[ X=\left[\begin{matrix} 0 & 1 \\ -1 & 0 \end{matrix}\right],\]
so that the group element can be written $O=I+d\theta X$, where $d\theta$ is some infinitesimal parameter of the group. To check the determinant we need to consider how we use the generator to recreate the group since the generator is defined through an infinitesimal change. For reasons that will become clearer later we use the exponential $O=\exp(\theta X)$, which to first order in an infinitesimal parameter $d\theta$ recreates the behaviour near the identity we used.
By the properties of matrix exponentiation we  can now check that $\det O=\det \exp(\theta X)=\exp(\theta \Tr X)=\exp(0)=1$. Since $X$ is an anti-Hermitian matrix, and we as physicists are more comfortable as Hermitian, we tend to use instead
\[ X=-i\left[\begin{matrix} 0 & 1 \\ -1 & 0 \end{matrix}\right]=\left[\begin{matrix} 0 & -i \\ i & 0 \end{matrix}\right],\]
as the generator, writing the group element as $O=\exp(i\theta X)$.

A similar calculation for $SO(3)$, which has three free parameters, gives the generators $X^i=\frac{1}{2}L^i$, where $L^i$ are the three matrices in Eq~(\ref{eq:SO3_generators}). These generators have the exact same commutator as the $SU(2)$ generators, $[X_i,X_j]=i\epsilon_{ijk}X^k$, indicating a structural similarity between the groups $SU(2)$ and $SO(3)$. For $SO(3)$ we can further find a corresponding set of differential generators
\begin{equation}
L_x=-i\left(y\frac{\partial}{\partial z}-z\frac{\partial}{\partial y} \right),\quad
L_y=-i\left(z\frac{\partial}{\partial x}-x\frac{\partial}{\partial z} \right),\quad
L_z=-i\left(x\frac{\partial}{\partial y}-y\frac{\partial}{\partial x} \right),
\label{eq:SO3_diff_generators}
\end{equation}
see Ex.~\ref{ex:SO2_diff_generators}.
Up to a constant factor of $\hbar$ you may recognise these operators as the angular momentum operators in quantum mechanics. This is not a coincidence, the angular momentum operators in quantum mechanics are a representation of the generators of the group of rotations in three dimensions. In fact, the possibility of going back and forth between differential representations and matrix representations of the Lie group is the essence of the duality between the Schr\"odinger and Heisenberg pictures of quantum mechanics.


%%%%%%%%%%%%%%%%
\subsection{Structure constants}
\label{sec:structure_constants}

In general, the commutators of the generators of a Lie group satisfy \[[X_i,X_j]=iC_{ij}^{~~k}X_k,\] where  $C_{ij}^{~~k}$ are called the {\bf structure constants} of the group.\footnote{There is an annoying difference in notation here between physics and mathematics, where the $i$ is commonly dropped. This has the same origin as the Hermitian versus anti-Hermitian operator used to create unitary operators discussed earlier.} 
This implies that the generators close under the commutation operation.

We can easily see that these commutators are antisymmetric in the indices $i$ and $j$, 
\[C_{ij}^{~~k} = -C_{ji}^{~~k}.\]
We can also show that there is a {\bf Jacobi identity} among the generators that just directly follows from the properties of the commutator,
\begin{equation}
[X_i, [X_j, X_k]] + [X_j, [X_k, X_i]] + [X_k, [X_i, X_j]] = 0,
\label{eq:Jacobi_generators}
\end{equation}
which in turn leads to a corresponding identity for the structure constants:
\begin{equation}
C_{il}^{~~m}C_{jk}^{~~l} + C_{jl}^{~~m}C_{ki}^{~~l} + C_{kl}^{~~m}C_{ij}^{~~l} = 0.
\end{equation}

Note that just like the generators are basis dependent, so are the structure constants. However, the closure in the commutation relationship is not. For example, we could have chosen the Pauli matrices to be the generators of $SU(2)$. They would then fulfil the commutator relationship
\begin{equation*}
[\sigma_i,\sigma_j]=2i\epsilon_{ijk}\sigma^k, 
\label{eq:pauli_algebra}
\end{equation*}
where the structure constant is $2\epsilon_{ijk}$ instead of $\epsilon_{ijk}$. For the $N$-dimensional unitary groups in physics it is common to use the following normalisation of the structure constants,
\begin{equation}
C_{kl}^{~~i}C_{}^{klj}=N\delta^{ij},
\label{eq:generator_normalisation}
\end{equation}
which coincides with the choices made above for $SU(2)$.



%%%%%%%%%
\section{Exercises}
%%%%%%%%%

%%%
\begin{Exercise}[]
Show that for the translation group $T(1)$ the identity element is $T_0$ and that $T_a^{-1} = T_{-a}$, and use this to show that $T(1)$ is group.
\end{Exercise}

\begin{Answer}
Given the group multiplication definition the identity element must be $e=T_0=1$ since $T_a\circ T_0=T_{a+0}=T_a$. Let $T_a^{-1} =T_b$. Since $T_a \circ T_b=T_{a+b}=1$, we find $b=-a$. (This does not show that the inverse is unique, but this is not a requirement.) We have now demonstrated the required existence of an identity element (iii) and an inverse (iv) for $T(1)$ as required by the group definition (the order of operations can obviously be reversed). The closure of the multiplication operation (i) is true by its definition. The associativity (ii) can be demonstrated as follows $(T_a\circ T_b)\circ T_c=T_{a+b}\circ T_c=T_{a+b+c}=T_{a}\circ T_{b+c}=T_a\circ (T_b\circ T_c)$.
\end{Answer}
%%%


%%%
\begin{Exercise}[]
Do the vectors in a vector space together with the vector addition operation form a group?
\end{Exercise}


\begin{Answer}
Yes. Vector addition is closed (the result is a new vector in the same space). Vector addition is associative. There is an identity in the null-vector, and the inverse is the negative of the vector.
\end{Answer}
%%%


%%%
\begin{Exercise}[]
Do the vectors in $\mathbb R^3$, together with the cross-product as the multiplication operation, from a group?
\end{Exercise}

\begin{Answer}
No. The cross product is not associative, among other things. (There is also no identity element since all cross products change the direction of the vectors.)
\end{Answer}
%%%

\begin{Exercise}[]
Show that the set of complex unitary $n\times n$ matrices $U$ with matrix multiplication is a group.
\end{Exercise}
\begin{Answer}
We need to demonstrate the four group requirements. We have closure since the product of two such unitary matrices $U_1$ and $U_2$ is a unitary matrix: $(U_1U_2)^\dagger U_1U_2= U_2^\dagger U_1^\dagger U_1U_2= U_2^\dagger U_2=I$. Matrix multiplication is always associative. There exists an identity among the matrices since the identity matrix is unitary: $I^\dagger I = II=I$. There exist an inverse $U^{-1}=U^\dagger$ for every $U$, since the matrix $U^\dagger$ is unitary if $U$ is: $(U^\dagger)^\dagger U^\dagger= UU^\dagger=I$.
\end{Answer}

\begin{Exercise}[]
What are the elements of $O(1)$? Can you find a group that it is isomorphic to?
\end{Exercise}
\begin{Answer}
Since $O(1)$ consists of $1\times1$ matrices, over $\mathbb R$ we suppose, also known as real numbers, the orthogonality requirement means that the transpose of that number (which is the number itself) multiplied by the number should be 1. There are two such reals, 1 and -1. The group thus has two elements. Since matrix multiplication in this case is just normal multiplication we have re-found the group $\mathbb Z_2$.
\end{Answer}

\begin{Exercise}[label=ex:Un_parameters]
Show that the $U(n)$ and $SU(n)$ groups have $n^2$ and $n^2-1$ independent (real) parameters, respectively. {\it Hint:} Consider the fact that $M=U^\dagger U$ is a hermitian matrix, {\it i.e.}\ $M^\dagger=M$.
\end{Exercise}
\begin{Answer}
An $n\times n$ matrix $U$ over $\mathbb C$ has $n^2$ complex or $2n^2$ real parameters. The hermitian matrix $M=U^\dagger U$ has $n$ real elements on the diagonal
and $(n^2-n)/2$ complex elements above and below the diagonal. Because $M^\dagger=M$ all the $(n^2-n)/2$ elements below the diagonal are given by the complex conjugate the corresponding elements above the diagonal. Since $U$ is unitary $M=I$. That all the $n$ real diagonal elements of $M$ is equal to one gives $n$ restrictions ($n$ real equations) for the elements of $U$. Further, all $(n^2-n)/2$ complex elements above the diagonal are zero, which gives $(n^2-n)/2$ complex equations, which means $n^2-n$ real equations. For the terms below the diagonal we do not obtain any new equations. As a result the free parameters for the $U(n)$ matrices are $2n^2-n-(n^2-n)=n^2$.
For $SU(n)$ we must use the additional requirement $\det U=1$. Because $1=\det M=\det (U^\dagger U)=\det U^\dagger \det U=|\det U|^2$, we see that $\det U$ must be a phase factor, and that the requirement $\det U=1$ only gives one new condition (equation). Thus there are $n^2-1$ independent real parameters in $SU(n)$.
\end{Answer}

\begin{Exercise}[]
Show that Eq.~(\ref{eq:SU2_parameterisation}) is a parametrisation of $SU(2)$.
\end{Exercise}
\begin{Answer}
A matrix in $SU(2)$ has $2^2-1=3$ free parameters. Let us start with a generic $2\times2$ complex valued matrix
\[ U=\left[ \begin{matrix} \alpha & \beta \\ \gamma & \delta \end{matrix} \right]. \]
For this to be unitary we must have 
\[ U^\dagger U=\left[ \begin{matrix} \alpha^* & \gamma^* \\ \beta^* & \delta^* \end{matrix} \right]\left[ \begin{matrix} \alpha & \beta \\ \gamma & \delta \end{matrix} \right] 
=\left[ \begin{matrix} |\alpha|^2+|\gamma|^2 & \alpha^*\beta+\gamma^*\delta \\ \beta^*\alpha+\delta^*\gamma & |\beta|^2+|\delta|^2 \end{matrix} \right]
=\left[ \begin{matrix} 1 & 0 \\ 0 & 1 \end{matrix} \right].\]
Then from the off-diagonal, $\delta^*\gamma=-\beta^*\alpha$ and $\gamma^*\delta=-\alpha^*\beta$ which gives $|\delta|^2|\gamma|^2= |\alpha|^2 |\beta|^2$, and inserting from the diagonal we have $(1-|\beta|^2)|\gamma|^2= (1-|\gamma|^2) |\beta|^2$ which can be solved for $|\gamma|^2= |\beta|^2$. Similarly from the diagonal $|\delta|^2=|\alpha|^2$. We can then generically  write $\gamma=e^{i\theta}\beta$ and $\delta=e^{i\phi}\alpha$, where $\theta$ and $\phi$ are two phases. The diagonal requirement is then completely fulfilled if $|\alpha|^2+|\beta|^2=1$ and the matrix is
\[ U=\left[ \begin{matrix} \alpha & \beta \\ e^{i\theta}\beta & e^{i\phi}\alpha \end{matrix} \right]. \]
The determinant then requires $\alpha e^{i\phi}\alpha-\beta e^{i\theta}\beta=1$ which is fulfilled for a phase $\phi$ that rotates $\alpha$ to  $\alpha^*$ and a phase $\theta$ that rotates $\beta$ to $-\beta^*$. Thus we can write
\[ U=\left[ \begin{matrix} \alpha & \beta \\ -\beta^* & \alpha^* \end{matrix} \right]. \]
\end{Answer}

\begin{Exercise}[]
Show that for a subset $H\subset G,$ if $h_i \circ h_j^{-1} \in H$ for all $h_i, h_j\in H$, then $H$ is a subgroup of $G$.
\end{Exercise}
\begin{Answer}
\end{Answer}

\begin{Exercise}[]
Show that if $H$ is a subgroup of $G$, then $h_i \circ h_j^{-1} \in H$ for all $h_i, h_j\in H$.
\end{Exercise}
\begin{Answer}
Since $H$ is a subgroup of $G$ then by point ii) in the definition since $h_j\in H$ we must also have $h_j^{-1} \in H$. With $h_i, h_j^{-1} \in H$ by point i) in definition $h_i  \circ h_j^{-1} \in H$.
\end{Answer}

\begin{Exercise}[label=ex:SUn_normal]
Show that $SU(n)$ is a proper subgroup of $U(n)$ and that $U(n)$ is not simple.
\end{Exercise}

\begin{Answer}
Let $U_i, U_j \in SU(n)$, then 
\[\det(U_i U_j^{-1}) = \det(U_i)\det(U_j^{-1}) = 1.\]
This means that $U_i U^{-1}_j \in SU(n)$. In other words, $SU(n)$ is a proper subgroup of $U(n)$. Let $V \in U(n)$ and $U \in SU(n)$, then $VUV^{-1} \in SU(n)$ because:
\[\det(VUV^{-1}) = \det(V)\det(U)\det(V^{-1}) = \frac{\det(V)}{\det(V)} \det(U) = 1.\]
In other words, $SU(n)$ is also a normal subgroup of $U(n)$, thus $U(n)$ is not simple.
\end{Answer}

\begin{Exercise}
Show any two (left or right) cosets are either disjoint or identical sets.
\end{Exercise}
\begin{Answer}
Let $H$ be a subgroup of $G$ and $g\in G$. For a member of the left coset $f\in gH$ then $gH=fH$ because $f\in gH$ implies that there must exist an $x\in H$ so that $gx=f$. Thus $fH=(gx)H=g(xH)$. Since $H$ is a group this means $xH=H$.  Thus every element in $G$ belong to exactly one left coset. The argument of right cosets is identical.
\end{Answer}

\begin{Exercise}[]
If $H$ is a normal subgroup of $G$, show that its left and right cosets are the same, and show that the set formed of the cosets is a group under an appropriate group operation.
\end{Exercise}

\begin{Exercise}[]
Show that the factors in a direct product of groups are normal groups to the product.
\end{Exercise}

\begin{Exercise}[]
Show that the group $G$ and the group $(G\times H) / H$ are isomorphic.
\end{Exercise}


\begin{Exercise}[]
Show that $U(1)\cong\mathbb{R}/\mathbb{Z}\cong SO(2)$.
\end{Exercise}


\begin{Exercise}[difficulty={2}]
Prove that all unitary representations on Hilbert spaces are completely reducible. {\it Hint:} For Hilbert spaces we always have an inner product defined.
\end{Exercise}
\begin{Answer}
Split the representation space $V=V_1\oplus V_2$, where we assume $V_1$ is a closed subspace of $V$ under the unitary representation $\rho(g)$ (the reducible part). Let $\mathbf v_1\in V_1$ and $\mathbf v_2\in V_2$.  Since $V_1$ is closed $\rho(g)\mathbf v_1 \in V_1$ and thus the inner product $(\rho(g)\mathbf v_1,  \mathbf v_2)=0$.  By unitarity the inner product is also $(\rho(g)\mathbf v_1,  \mathbf v_2)=(\mathbf v_1, \rho(g)^{-1}\mathbf v_2)$, thus $\rho(g)^{-1}V_2 \subset V_2$ and both subspaces are closed, and the space thus completely reducible. 
\end{Answer}


\begin{Exercise}[difficulty={1}]
Use the parameterisation of $SO(2)$ in (\ref{eq:SO2_parameterisation}) to find the generator of the group using generating functions and show that you get the same answer as when using the infinitesimal matrix demonstrated in the notes.
\end{Exercise}


\begin{Exercise}[]
Find the fundamental matrix representation for $SO(3)$.
\end{Exercise}


\begin{Exercise}[difficulty={2},label=ex:SO2_diff_generators]
Find a differential representation of the generators for $SO(3)$ by studying how a vector in $\mathbb R^3$ changes under a parameterised $SO(3)$ transformation. {\it Hint}: Depending on the choice of parametrisation you may also need to use the commutator relationships between the generators.
\end{Exercise}
\begin{Answer}
Finding the differential generators for SO(3) begins with finding the composition function, or more specifically the elements of $\mathbf  x'=\mathbf f(\mathbf x, \boldsymbol \Lambda)$ where $\mathbf x$ is a vector in $\mathbb{R}^3$, $\boldsymbol \Lambda$ are the parameters of the transformation (the parameterisation of the manifold of the Lie group), and $\mathbf  x'$ the transformed vector. Using the parameterisation of rotations in $\mathbb{R}^3$ given in Eq.~(\ref{eq:R3_rotation_Euler}) ur composition function becomes 
$$f_i(\mathbf  x,\boldsymbol \Lambda) = R(\alpha,\beta,\gamma)_{ij} x_j,$$ 
with $\boldsymbol\Lambda= (\alpha,\beta,\gamma)$ containing the angles which we rotate around the different axis. Explicitly, the components thus become
\begin{align}
f_1(\mathbf  x,\boldsymbol \Lambda) &= (\cos\alpha\cos\beta\cos\gamma - \sin\alpha\sin\gamma)x - (\cos\alpha\cos\beta\sin\gamma+\sin\theta\cos\gamma)y + \cos\alpha\sin\beta z \\
f_2(\mathbf  x,\boldsymbol\Lambda) &= (\sin\alpha\cos\beta\cos\gamma + \cos\alpha\sin\gamma)x + (\cos\alpha\cos\gamma-\sin\alpha\cos\beta\sin\gamma)y + \sin\alpha\sin\beta z\\
f_3(\mathbf  x,\boldsymbol\Lambda) &= -\sin\beta\cos\gamma x + \sin\beta\sin\gamma y + \cos\beta z
\end{align}

The next step is finding the generators $X^j$ from the definition
$$iX^j = \left.\frac{\partial f_i}{\partial\Lambda_j}\frac{\partial}{\partial x_i}\right|_{\boldsymbol\Lambda = 0}.$$
To simplify notation we abbreviate $\frac{\partial}{\partial x} =\partial_x$. Doing the calculation for all three parameters of the manifold we get three generators, as follows. 
$$iX^1 = x\partial_y-y\partial_x \Rightarrow L_z\equiv X^1 = -i(x\partial_y-y\partial_x),$$
$$iX^2 = z\partial_x-x\partial_z \Rightarrow L_y\equiv X^2 = -i(z\partial_x-x\partial_z),$$
$$iX^3 = x\partial_y-y\partial_x \Rightarrow L_z\equiv X^3 = -i(x\partial_y-y\partial_x).$$
The generators we found are proportional to (two of) the angular momentum operators in quantum mechanics.
Unfortunately, only two of the generators are unique. This is due to the choices made in the parametrisation of the manifold, and will depend on which parametrisation you start with. To find the last generator of our group, all we need to do here is to use the commutator relations between the generators. We start with
\begin{eqnarray*}
\left[L_y,L_z\right]f &=& -(z\partial_x-x\partial_z)(x\partial_y-y\partial_x)f + (x\partial_y-y\partial_x)(z\partial_x-x\partial_z)f \\
&=& -z\partial_y f -zx\partial_x\partial_y f + zy\partial_x^2 f+x^2\partial_z\partial_y f - xy\partial_z\partial_x f \\
&&+ xz\partial_y\partial_x f-x^2\partial_y\partial_z f-yz\partial_x^2 f+y\partial_zf+yx\partial_x\partial_z f \\
&=& i(-i)(y\partial_z-z\partial_y)f,
\end{eqnarray*}
where we have use that the partial derivatives commute. Since we know from our commutators that $\left[L_y,L_z\right]=iL_x$, our last generator becomes $$L_x=-i(y\partial z-z\partial y).$$
 %The last two commutators are $$\left[L_x,L_y\right] = -y\partial_x+x\partial_y = i(-i)(x\partial_y-y\partial_x) = iL_z.$$
%$$\left[L_z,L_x\right] = -x\partial_z+z\partial_x = i(-i)(z\partial_x-x\partial_z) = iL_y.$$
\end{Answer}

\begin{Exercise}[difficulty={3},label=ex:SU2_generators]
Find an expression for the generators of $SU(2)$ and their commutation relationships. {\it Hint:} One answer uses a composition function but this approach has some dangers, try also to derive the properties of the generators from a member of the group an infinitesimal distance from the identity.
\end{Exercise}
\begin{Answer}
A member $U$ of $SU(2)$ fulfils $UU^\dagger=I$ and has $\det U =1$ and can be written as 
\[U=\left[\begin{matrix} a+ib & c+id \\ -c+id &  a-ib \end{matrix}\right],\] 
where $a,b,c,d\in\mathbb R$ and $a^2+b^2+c^2+d^2=1$. Using a composition function here it is tempting to solve for $a=\sqrt{1-b^2-c^2-d^2}$ and let $a,b,c$ be the three free parameters if $SU(2)$. Note that solving for $b$, $c$ or $d$ is not really an option as a requirement for deriving the generators from the composition function is that the zero parameters, here $b=c=d=0$, give the identity element of the group, which is the identity matrix. However, there is a potential issue here that choosing the sign on the square root restricts the parameterisation to apply for only group members with positive real components in the upper left element, meaning that this composition function only works for part of the group. This is another example of a local description of the group.  Other parameterisations exist, however, they may have other problems such as a non-unique zero element (meaning no inverse of the composition function exists).

Fortunately, this parameterisation is enough to give all of the generators of the whole group:
\begin{eqnarray*} 
X_1&=&\left.\frac{\partial U}{\partial  d}\right|_{b=c=d=0}=\left[\begin{matrix} 0 & i \\ i & 0 \end{matrix}\right], \\
X_2&=&\left.\frac{\partial U}{\partial  c}\right|_{b=c=d=0}=\left[\begin{matrix} 0 & 1 \\ -1 & 0 \end{matrix}\right], \\
X_3&=&\left.\frac{\partial U}{\partial  b}\right|_{b=c=d=0}=\left[\begin{matrix} i & 0 \\ 0 & -i \end{matrix}\right].
\end{eqnarray*} 
While these are not the Pauli matrices we might be expecting, they are related to the Pauli matrices by a simple multiplicative factor $\sigma_1=-iX_1$,  $\sigma_2=-iX_2$,  and $\sigma_3=-iX_3$. In fact the matrices we have found are typically used in mathematical texts as the generators of $SU(2)$, but then the exponential map has no factor of $i$. Notice that these matrices are anti-hermitian, while the Pauli matrices are hermitian.

The infinitesimal approach writes the group member as $U=e^{ida_iX_i}\simeq I+ida_i X_i$, and applying the unitarity requirement 
\[UU^\dagger\simeq(I+ida_i X_i)(I+ida_i X_i)^\dagger=I+ida_i(X_i-X_i^\dagger)+\ldots,\]
means that a necessary condition on the generators $X_i$ is that they are hermitian $X_i=X_i^\dagger$. The general form of the generators is thus restricted to
\[X=\left[\begin{matrix} \alpha & \gamma \\ \gamma^* &   \beta  \end{matrix}\right],\] 
where  $\alpha,\beta\in\mathbb R$  and $\gamma\in\mathbb C$. The property of the determinant can be found as follows (see Exercise~\ref{ex:expmap_prop}):
\[\det U =\det e^{ida_iX_i}=e^{\tr{ida_iX_i}}=e^{ida_i\tr{X_i}}.\]
This means that for the determinant to be one, each trace must be zero, meaning that the generators are traceless so that the general form of the generators is restricted to
\[X=\left[\begin{matrix} \alpha & \gamma \\ \gamma^* &   -\alpha  \end{matrix}\right].\]
From the requirement $\alpha^2+|\gamma|^2=1$ we can recreate the three Pauli matrices by the three choices: 1) $\alpha=1$, $\gamma=0$, 2) $\alpha=0$, $\gamma=1$, and 3) $\alpha=0$, $\gamma=-i$. Naturally, there is a continuum of equivalent expressions for the generators from other compatible choices. 

This approach emphasises two important properties of the generators. Firstly that the generators are hermitian. Since the generators will function as operators in a QM of QFT setting, this is very desirable. Second, the generators are traceless.  This will often be a calculational advantage.
\end{Answer}


\begin{Exercise}[]
What are the structure constants of SU(2)?
\end{Exercise}


\begin{Exercise}[]
Find the adjoint representation for $SU(2)$. Compare this to the fundamental representation of $SO(3)$.
\end{Exercise}



\begin{Exercise}[]
Let $A$ be an algebra based on a finite-dimensional vector space over a field $F$ ,with a basis $B=\{b_i | i=1,\ldots,n\}$. Show that the multiplication of elements in $A$ is completely determined by the $n^2$ products $b_ib_j$ for each pair of basis vectors in $B$. 
\end{Exercise}


\begin{Exercise}[]
Let $V$ be a finite-dimensional vector space over a field $F$ with a basis $B=\{b_i | i=1,\ldots,n\}$. Let  $\{c_{rst}| r,s,t=1,\ldots,n\}$ be a collection of $n^3$ elements in $F$. Show that there exists one, and only one, multiplication operation on $V$ so that $V$ is an algebra over $F$ under this multiplication and
\[ b_rb_s = c_{rst}b_t,\]
for every pair of basis vectors in $B$. The elements $c_{rst}$ are the structure constants of the algebra.
\end{Exercise}


\begin{Exercise}[]
Show that $\mathbb{R}^3$ with the binary operator $[\mathbf x, \mathbf y]=\mathbf x \times \mathbf y$ is a Lie algebra.
\end{Exercise}

\begin{Exercise}[]
Show Eq.~(\ref{eq:adjoint_rep_commutator}).
\end{Exercise}

\begin{Exercise}[]
Let $A_i$ be the generators of the group $G$ and $B_j$ be the generators of group $H$. Explain in what sense the $A_i$ and $B_j$ are generators of the direct product group $G\times H$ and show that $[A_i,B_j]=0$.
\end{Exercise}


\begin{Exercise}[title={Properties of matrix exponentiation},label={ex:expmap_prop}]
Prove the following useful properties of matrix exponentiation. $A$ and $B$ are matrices.
\begin{enumerate}
\item If $A$ and $B$ commute, $e^{A+B}=e^Ae^B$.
\item If $B$ has an inverse, $e^{BAB^{-1}}=Be^AB^{-1}$.
\item $e^{A^*}=(e^A)^*$
\item $e^{A^T}=(e^A)^T$
\item $e^{A^\dagger}=(e^A)^\dagger$
\item $e^{-A}=(e^A)^{-1}$
\item $\det e^A=e^{\tr{A}}$.
\end{enumerate}
\end{Exercise}



\end{document}
